---
title: "Santander Product Recommendation"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Competition Solution

This is a detailed description of our best solution to the [Santander Product Recommendation](https://www.kaggle.com/c/santander-product-recommendation) competition hosted by [Kaggle](http://www.kaggle.com). The challenge was to produce a recommendation system for Santander Banks. They provide a number of financial services (savings accounts, credit cards, etc), and the competition dataset included ~13.6 million records of their customers over a period over a 1.5 year period. The goal was to predict which new products a customer would purchase, if any, in the next month, which was withheld. For this competition I teamed up with my good friend and fellow data scientist Matt -- you can check out his blog [here](http://statmills.com). Matt did a lot of useful statistical/correlative analysis of purchasing trends which I used to build the final machine learning models.

### Solution Summary

Our solution was a collection of 22 XGBoost trained on data from June 2015 and December 2015 for only accounts that added products with an additional NUMBER-HERE engineered features (described below). Really it was just one XGBoost model that was trained once for each of the 22 products in turn as the target label. Technically there were 24 products, but two of them, `ind_aval_fin_ult1` and `ind_ahor_fin_ult1`, were discarded for being exceedingly rare. The resulting probabilities for each of the 22 remaining products in June 2016 were sorted, and for each account the top 7 products that were not owned in May 2016 were recommended as the final solution.  

There were a couple of key insights that were critical to sucess in this competition. The first was limiting the dataset to only accounts that actually added any products. Note that the challenge is to predict which products a customer will by *if any*. We don't need to determine the likelihood that they actually make the purchase, and as the final score is evaluated using mean average precision ([MAP@7](https://www.kaggle.com/wiki/MeanAveragePrecision)), any recommendations made to accounts that don't purchase anything does not affect the score negatively. Adding products is a rare event and limiting only to entries where products were added reduces the dataset size from about 1 million entries per month to a much more reasonable 20-40,000.

Second, we found that the testing month, June, is special in Spain because that is when taxes are due. The result is that product purchase trends are quite different for June, particularly for tax services (d'oh). A helpful visualization of this was [provided by Russ W](https://www.kaggle.com/c/santander-product-recommendation/forums/t/25629/product-month-stacked-bar-chart). Training on data from June 2015 for accounts that added products with just the original features plus an additional one indicating whether or not each product was owned last month (which you need anyway to determine whether or not each product could have been added in the first place) produced a MAP@7 of ~0.027, which would have placed around top 35%.

The third realization was that the most useful features were those related to product ownership. After looking at the feature importance outputted  by XGBoost, I added the product ownership status for each of the previous 2-5 months as features in addition to the most recent month and applied the above strategy to produce a MAP@7 greater than 0.03, which would have resulted in a top 100 finish and a bronze medal. These lagged ownership features were limited to 5 months because the earliest data was from Jan. 2015 and thus this was the furthest back we could go. If I were actually being contracted by Santander to build a recommendation system, this is likely where I would stop. Estimates of the maximum possible score [were around 0.035](https://kaggle2.blob.core.windows.net/forum-message-attachments/146271/5405/Private%20maximum%20score.png?sv=2015-12-11&sr=b&sig=c4ljinra%2F8PXV85C8TuSBfHM3CeyrOKkKXDhI01yJIY%3D&se=2016-12-19T18%3A27%3A15Z&sp=r), so with this fairly simple approach we have already achieved more than 85% of the maximum possible precision. The additional time invested and model complexity added to increase performance from 0.03 to 0.0305 (HOW MANY PLACES) was significant. But this is a competition so we press on.

The decision to train on December 2015 resulted from my trying to engineer more features related to product ownership. I figured there is a trade-off between 1) the advantage of capturing June-specific trends by training on June and 2) capturing product ownership specific trends by training on later months that have a longer history. I was in the process of exploring which months had similar purchasing patterns to June when Kaggler [AMZ](https://www.kaggle.com/albertomz) posted [this analysis](https://www.kaggle.com/c/santander-product-recommendation/forums/t/26138/interesting-relationship-between-june-and-december?forumMessageId=148422). He found December to be most similar to June and saved me some time, so many thanks to him for that. Training examples in December can contain up to 11 months of historical ownership features, the inclusion of which improved precision to ~0.0304 (TOP WHAT PERCENT??).

The final improvement to WHAT SCORE? was the result of the following additional features (mostly product related):

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
